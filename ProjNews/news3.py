import streamlit as st
import pandas as pd
import feedparser
from urllib.parse import quote
from bs4 import BeautifulSoup
from konlpy.tag import Okt
from sentence_transformers import SentenceTransformer, util
from transformers import pipeline
from wordcloud import WordCloud
from io import BytesIO
import matplotlib.pyplot as plt
import plotly.express as px
from datetime import datetime
import os
import requests
import newspaper
from newspaper import Article
import nltk
from fpdf import FPDF

nltk.download('punkt')

# ëª¨ë¸ ë° ë¼ì´ë¸ŒëŸ¬ë¦¬ ì´ˆê¸°í™”
@st.cache_resource
def load_models():
    summarizer = SentenceTransformer("jhgan/ko-sroberta-multitask")
    sentiment_ko = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")
    sentiment_en = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")
    okt = Okt()
    return summarizer, sentiment_ko, sentiment_en, okt

summarizer, sentiment_ko, sentiment_en, okt = load_models()

# ê¸°ë³¸ UI ì„¤ì •
st.set_page_config(page_title="ğŸ“° AI/ë¡œë´‡ ë‰´ìŠ¤ ëŒ€ì‹œë³´ë“œ", layout="wide")
st.title("ğŸ§ ğŸ› ï¸ğŸ•¹ï¸ AI/ë¡œë´‡ ë‰´ìŠ¤ ìš”ì•½ ëŒ€ì‹œë³´ë“œ")
st.caption("ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§")

# í•„í„° ë° ì˜µì…˜
KEYWORDS = ["AI", "ë¡œë´‡", "ë¡œë´‡ê°ì •", "ë¡œë´‡ì„±ê²©", "IT", "ì‚°ì—…ë°ì´í„°", "ë°ì´í„°ì‹œìŠ¤í…œ"]
selected_keywords = st.sidebar.multiselect("ğŸ” ê´€ì‹¬ í‚¤ì›Œë“œ", KEYWORDS, default=KEYWORDS)
extra_kw = st.sidebar.text_input("â• ì¶”ê°€ í‚¤ì›Œë“œ (ì‰¼í‘œêµ¬ë¶„)")
if extra_kw:
    selected_keywords += [kw.strip() for kw in extra_kw.split(",") if kw.strip()]
lang_option = st.sidebar.radio("ğŸŒ ë‰´ìŠ¤ ì–¸ì–´", ["í•œêµ­ì–´", "ì˜ì–´"])
max_items = st.sidebar.slider("ğŸ“° í‚¤ì›Œë“œë³„ ìµœëŒ€ ë‰´ìŠ¤ ìˆ˜", 1, 15, 5)
start_date = st.sidebar.date_input("ğŸ“… ì‹œì‘ì¼", None)
end_date = st.sidebar.date_input("ğŸ“… ì¢…ë£Œì¼", None)
view_type = st.sidebar.radio("ë‰´ìŠ¤ ë³´ê¸° ë°©ì‹", ["ì¹´ë“œë·°", "í…Œì´ë¸”"])
show_network = st.sidebar.checkbox("ğŸ“Š í‚¤ì›Œë“œ-ê°ì„± ë„¤íŠ¸ì›Œí¬ ê·¸ë˜í”„", False)
show_memo = st.sidebar.checkbox("ğŸ“ ë¶ë§ˆí¬ ë©”ëª¨ì“°ê¸° í™œì„±í™”", True)

if "bookmarks" not in st.session_state:
    st.session_state["bookmarks"] = []
if "memo" not in st.session_state:
    st.session_state["memo"] = dict()

# í…ìŠ¤íŠ¸ ì •ë¦¬ í•¨ìˆ˜
def clean_text(html):
    return BeautifulSoup(html, "html.parser").get_text(separator=" ").strip()

# ë‰´ìŠ¤ ë³¸ë¬¸ í¬ë¡¤ë§ & í…ìŠ¤íŠ¸ ì¶”ì¶œ
@st.cache_data(show_spinner=True)
def get_article_text(url):
    try:
        article = Article(url, language='ko' if lang_option == "í•œêµ­ì–´" else 'en')
        article.download()
        article.parse()
        article.nlp()
        return article.text
    except Exception:
        return ""

# AI ìš”ì•½ í•¨ìˆ˜
def summarize(text, num_sent=3):
    if not text or len(text) < 10:
        return ""
    sentences = text.replace("!", ".").split(". ")
    if len(sentences) <= num_sent:
        return text
    emb = summarizer.encode(sentences)
    center = emb.mean(axis=0)
    scores = [util.pytorch_cos_sim([center], [e])[0][0] for e in emb]
    top_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:num_sent]
    return ". ".join([sentences[i] for i in sorted(top_idx)])

# í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords(text, n=5):
    nouns = [w for w in okt.nouns(text) if len(w) > 1]
    freq = pd.Series(nouns).value_counts() if nouns else pd.Series([])
    return ", ".join(freq[:n].index) if not freq.empty else ""

# ê°ì„± ë¶„ì„
def get_sentiment(text, lang="ko"):
    try:
        if lang == "ko":
            result = sentiment_ko(text[:256])
            return result[0]["label"]
        else:
            result = sentiment_en(text)
            return {"POSITIVE": "ê¸ì •", "NEGATIVE": "ë¶€ì •"}.get(result[0]["label"], "ì¤‘ë¦½")
    except:
        return "ì¤‘ë¦½"

# ë‰´ìŠ¤ RSSì—ì„œ ì œëª© ë“± ê¸°ë³¸ ìˆ˜ì§‘ + ë³¸ë¬¸ ë³„ë„ í¬ë¡¤ë§
@st.cache_data(show_spinner=True)
def fetch_news(keyword, lang="ko"):
    q = quote(keyword)
    rss_url = f"https://news.google.com/rss/search?q={q}&hl={'ko' if lang=='ko' else 'en'}&gl=KR&ceid=KR:{'ko' if lang=='ko' else 'en'}"
    feed = feedparser.parse(rss_url)
    articles = []
    for entry in feed.entries[:max_items]:
        try:
            title = entry.title
            link = entry.link
            date = entry.get("published", datetime.now().strftime("%Y-%m-%d"))
            raw_summary = entry.summary if hasattr(entry, "summary") else title
            summary = clean_text(raw_summary)

            # ë³¸ë¬¸ í¬ë¡¤ë§ ì¶”ê°€
            article_text = get_article_text(link)
            if article_text:
                summary = article_text  # ë³¸ë¬¸ ìš°ì„  ì‚¬ìš©

            # ìš”ì•½, í‚¤ì›Œë“œ, ê°ì„±
            summ = summarize(summary, num_sent=3)
            kws = extract_keywords(summary)
            senti = get_sentiment(summary, lang=lang)

            articles.append({
                "í‚¤ì›Œë“œ": keyword, "ì œëª©": title, "ë§í¬": link, "ë‚ ì§œ": date,
                "ë³¸ë¬¸": summary, "ìš”ì•½": summ, "í‚¤ì›Œë“œì¶”ì¶œ": kws, "ê°ì„±": senti
            })
        except Exception as e:
            continue
    return pd.DataFrame(articles)

# ì „ì²´ ë‰´ìŠ¤ ìˆ˜ì§‘ ë° í•„í„°ë§
df_list = [fetch_news(k, lang="ko" if lang_option=="í•œêµ­ì–´" else "en") for k in selected_keywords]
news_df = pd.concat(df_list).drop_duplicates(subset=["ë§í¬"])
news_df["ë‚ ì§œ"] = pd.to_datetime(news_df["ë‚ ì§œ"], errors="coerce")

if start_date:
    news_df = news_df[news_df["ë‚ ì§œ"] >= pd.to_datetime(start_date)]
if end_date:
    news_df = news_df[news_df["ë‚ ì§œ"] <= pd.to_datetime(end_date)]

# ê°ì„± ì´ëª¨ì§€ ë§¤í•‘
senti_emoji = {"ê¸ì •": "ğŸŸ¢", "ë¶€ì •": "ğŸ”´", "ì¤‘ë¦½": "ğŸŸ¡"}
news_df["ê°ì„±ì´ëª¨ì§€"] = news_df["ê°ì„±"].map(senti_emoji)

st.markdown(f"### ì´ ë‰´ìŠ¤ ê±´ìˆ˜: {len(news_df)}ê±´")
if news_df.empty:
    st.warning("ì¡°ê±´ì— ë§ëŠ” ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    st.stop()

# ì‹œê³„ì—´, ê°ì„± ë¶„í¬, í‚¤ì›Œë“œ ë¹ˆë„
with st.expander("ğŸ§® ë‰´ìŠ¤ í†µê³„ ë° íŠ¸ë Œë“œ", expanded=True):
    col1, col2, col3 = st.columns(3)
    with col1:
        st.subheader("í‚¤ì›Œë“œë³„ ë‰´ìŠ¤ ê±´ìˆ˜")
        st.bar_chart(news_df["í‚¤ì›Œë“œ"].value_counts())
    with col2:
        st.subheader("ğŸ‘ğŸ‘ ê°ì„± ë¶„í¬")
        fig = px.pie(news_df, names="ê°ì„±", title="ê°ì„± ë¶„í¬")
        st.plotly_chart(fig, use_container_width=True)
    with col3:
        st.subheader("ğŸ”¥ ì¸ê¸° í‚¤ì›Œë“œ Top10")
        all_kws = ", ".join(news_df["í‚¤ì›Œë“œì¶”ì¶œ"].dropna())
        kws = [k.strip() for k in all_kws.split(",") if k.strip()]
        kw_freq = pd.Series(kws).value_counts().head(10)
        st.bar_chart(kw_freq)

    emo_daily = news_df.groupby([news_df["ë‚ ì§œ"].dt.date, "ê°ì„±"]).size().unstack().fillna(0)
    st.subheader("ğŸ“† ì¼ë³„ ê°ì„± ì‹œê³„ì—´")
    st.line_chart(emo_daily)

# í‚¤ì›Œë“œë³„ ê°ì„± ë¹„êµ
with st.expander("ğŸ†š í‚¤ì›Œë“œë³„ ê°ì„± ë¹„êµ"):
    selected = st.multiselect("ë¹„êµí•  í‚¤ì›Œë“œ ì„ íƒ", news_df["í‚¤ì›Œë“œ"].unique())
    if selected:
        df_comp = news_df[news_df["í‚¤ì›Œë“œ"].isin(selected)]
        fig = px.histogram(df_comp, x="ê°ì„±", color="í‚¤ì›Œë“œ", barmode="group", title="í‚¤ì›Œë“œë³„ ê°ì„± ë¹„êµ")
        st.plotly_chart(fig)

# ë‰´ìŠ¤ ëª©ë¡ ì¶œë ¥ ë° ë¶ë§ˆí¬ + ë©”ëª¨
st.markdown("## ë‰´ìŠ¤ ëª©ë¡")
for idx, row in news_df.iterrows():
    key = f"bm_{hash(row['ë§í¬'])}"
    with st.container():
        st.markdown(f"### [{row['ì œëª©']}]({row['ë§í¬']})")
        st.caption(f"ğŸ“… {row['ë‚ ì§œ'].date()}")
        st.markdown(f"ğŸ§¾ {row['ìš”ì•½']}")
        st.markdown(f"ê°ì„±: {senti_emoji.get(row['ê°ì„±'], 'ğŸŸ¡')} {row['ê°ì„±']}")
        st.markdown(f"`{row['í‚¤ì›Œë“œì¶”ì¶œ']}`")
        if row['ê°ì„±'] == "ë¶€ì •":
            st.warning("ğŸš¨ ë¶€ì •ì  ë‰´ìŠ¤ì…ë‹ˆë‹¤.")
        if st.button("â­ ë¶ë§ˆí¬", key=key):
            if row["ë§í¬"] not in st.session_state["bookmarks"]:
                st.session_state["bookmarks"].append(row["ë§í¬"])
        if show_memo:
            memo_key = f"memo_{key}"
            memo_text = st.text_area("ë©”ëª¨", st.session_state.get(memo_key, ""),
                                    key=memo_key, height=60)

# ë¶ë§ˆí¬í•œ ë‰´ìŠ¤ PDF ì €ì¥
with st.expander("ğŸ“¥ ë¶ë§ˆí¬ ëª¨ì•„ë³´ê¸° & PDF ì €ì¥"):
    bm_df = news_df[news_df["ë§í¬"].isin(st.session_state["bookmarks"])]
    if len(bm_df):
        for _, row in bm_df.iterrows():
            st.markdown(f"- [{row['ì œëª©']}]({row['ë§í¬']})")
        if st.button("ë¶ë§ˆí¬ ë‰´ìŠ¤ PDF ì €ì¥"):
            pdf = FPDF()
            pdf.add_page()

            # í•œê¸€ ìœ ë‹ˆì½”ë“œ ì§€ì› í°íŠ¸ íŒŒì¼ ê²½ë¡œ (í™•ì¥ì .ttfê°€ ë¹ ì¡Œìœ¼ë©´ ì¶”ê°€í•˜ì„¸ìš”)
            font_path = r"C:\Users\EDU03-17\Downloads\Noto_Sans_KR\static\NotoSansKR-Regular.ttf"

            # ìœ ë‹ˆì½”ë“œ(TTF) í°íŠ¸ ë“±ë¡
            pdf.add_font('NotoSansKR', '', font_path, uni=True)
            pdf.set_font('NotoSansKR', '', 12)

            pdf.cell(200, 10, "ë¶ë§ˆí¬ ë‰´ìŠ¤ ìš”ì•½", 0, 1, 'C')

            for _, row in bm_df.iterrows():
                key = f"bm_{hash(row['ë§í¬'])}"
                memo_key = f"memo_{key}"
                memo = st.session_state.get(memo_key, "")

                entry = (
                    "ğŸ“° ì œëª© : " + row['ì œëª©'] + "\n"
                    + "ğŸ§¾ ìš”ì•½ : " + row['ìš”ì•½'] + "\n"
                    + "ğŸ·ï¸ í‚¤ì›Œë“œ : " + row['í‚¤ì›Œë“œì¶”ì¶œ'] + "\n"
                    + "â¤ï¸ ê°ì„± : " + row['ê°ì„±'] + " " + senti_emoji.get(row['ê°ì„±'], "") + "\n"
                    + (f"ğŸ“ ë©”ëª¨ : {memo}\n" if memo else "")
                    + "ğŸ”— ë§í¬ : " + row['ë§í¬'] + "\n"
                    + "-"*50 + "\n"
                )
                pdf.multi_cell(0, 10, entry)

            temp = BytesIO()
            pdf.output(temp)
            st.download_button("â¬‡ï¸ PDF ë‹¤ìš´ë¡œë“œ", data=temp.getvalue(), file_name="ë¶ë§ˆí¬_ë‰´ìŠ¤.pdf")
    else:
        st.info("ğŸ“Œ ë¶ë§ˆí¬ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤. ë‰´ìŠ¤ ì˜† ë²„íŠ¼ìœ¼ë¡œ ë¶ë§ˆí¬í•˜ì„¸ìš”.")