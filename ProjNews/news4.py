import streamlit as st
import pandas as pd
import feedparser
from urllib.parse import quote
from bs4 import BeautifulSoup
from konlpy.tag import Okt
from sentence_transformers import SentenceTransformer, util
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import plotly.express as px
from datetime import datetime
from newspaper import Article
import nltk
from fpdf import FPDF
from io import BytesIO

nltk.download('punkt')

@st.cache_resource
def load_models():
    summarizer = SentenceTransformer("jhgan/ko-sroberta-multitask")
    okt = Okt()
    return summarizer, okt

summarizer, okt = load_models()

# í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ë‰´ìŠ¤ ìš”ì•½ ëŒ€ì‹œë³´ë“œ", layout="wide")
st.title("ğŸ§ ğŸ› ï¸ğŸ•¹ï¸ AI/ë¡œë´‡ ë‰´ìŠ¤ ìš”ì•½ ëŒ€ì‹œë³´ë“œ")

# ì‚¬ì´ë“œë°”
st.sidebar.header("ğŸ” ë‰´ìŠ¤ ìˆ˜ì§‘ ì¡°ê±´")
KEYWORDS = ["AI", "ë¡œë´‡", "ë¡œë´‡ê°ì •", "ë¡œë´‡ì„±ê²©", "IT", "ì‚°ì—…ë°ì´í„°", "ë°ì´í„°ì‹œìŠ¤í…œ"]
selected_keywords = st.sidebar.multiselect("í‚¤ì›Œë“œ ì„ íƒ", KEYWORDS, default=KEYWORDS)
extra_kw = st.sidebar.text_input("â• ì¶”ê°€ í‚¤ì›Œë“œ (ì‰¼í‘œ êµ¬ë¶„)")
if extra_kw:
    selected_keywords += [kw.strip() for kw in extra_kw.split(",") if kw.strip()]
lang_option = st.sidebar.radio("ğŸŒ ë‰´ìŠ¤ ì–¸ì–´", ["í•œêµ­ì–´", "ì˜ì–´"])
max_items = st.sidebar.slider("ğŸ“° í‚¤ì›Œë“œë³„ ë‰´ìŠ¤ ìˆ˜", 1, 15, 5)
start_date = st.sidebar.date_input("ğŸ“… ì‹œì‘ì¼", None)
end_date = st.sidebar.date_input("ğŸ“… ì¢…ë£Œì¼", None)

# ë‰´ìŠ¤ ë³¸ë¬¸ ì •ì œ
def clean_text(html):
    return BeautifulSoup(html, "html.parser").get_text(separator=" ").strip()

# ìš”ì•½
def summarize(text, num_sent=3):
    if not text or len(text.strip()) < 30:
        return "ìš”ì•½ ë¶ˆê°€ (ë³¸ë¬¸ ë¶€ì¡±)"
    sentences = [s.strip() for s in text.replace("!", ".").split(". ") if len(s.strip()) > 10]
    if len(sentences) <= num_sent:
        return text
    emb = summarizer.encode(sentences, convert_to_tensor=True)
    center = emb.mean(dim=0)
    scores = [util.pytorch_cos_sim(center, e)[0][0].item() for e in emb]
    top_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:num_sent]
    return ". ".join([sentences[i] for i in sorted(top_idx)])

# í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords(text, n=5):
    nouns = [w for w in okt.nouns(text) if len(w) > 1]
    freq = pd.Series(nouns).value_counts()
    return ", ".join(freq.head(n).index) if not freq.empty else "í‚¤ì›Œë“œ ì—†ìŒ"

# ì½˜í…ì¸  í†¤ ë¶„ì„
def analyze_tone(text):
    if len(text) > 1000 and any(word in text for word in ["ìš°ë ¤", "ë…¼ë€", "í™˜ì˜", "ë…¼ì˜", "ê³¼ì œ"]):
        return "ë¶„ì„ì "
    elif any(word in text for word in ["ìŠ¬í”„ë‹¤", "ê¸°ì˜ë‹¤", "ì¶©ê²©", "ê°ë™", "ë¶„ë…¸"]):
        return "ê°ì •ì "
    else:
        return "ì •ë³´ì„±"

# íƒœê·¸ ìƒì„±
def generate_tags(text):
    tags = []
    if "ê¸°ìˆ " in text:
        tags.append("#ê¸°ìˆ ë™í–¥")
    if "ì‹œì¥" in text or "ìˆ˜ìš”" in text:
        tags.append("#ì‹œì¥ë¶„ì„")
    if "ë…¼ë€" in text or "ë¬¸ì œ" in text:
        tags.append("#ì´ìŠˆ")
    return " ".join(tags) if tags else "#ì¼ë°˜"

# ë‰´ìŠ¤ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
@st.cache_data(show_spinner=True)
def get_article_text(url, lang="ko"):
    try:
        article = Article(url, language='ko' if lang == "í•œêµ­ì–´" else 'en')
        article.download()
        article.parse()
        return article.text.strip()
    except:
        return ""

# ë‰´ìŠ¤ í¬ë¡¤ë§
@st.cache_data(show_spinner=True)
def fetch_news(keyword, lang="ko", max_items=5):
    q = quote(keyword)
    rss_url = f"https://news.google.com/rss/search?q={q}&hl={lang}&gl=KR&ceid=KR:{lang}"
    feed = feedparser.parse(rss_url)
    articles = []
    for entry in feed.entries[:max_items]:
        try:
            title = entry.title
            link = entry.link
            date = entry.get("published", datetime.now().strftime("%Y-%m-%d"))
            preview = clean_text(entry.summary if hasattr(entry, "summary") else "")
            fulltext = get_article_text(link, lang)
            if not fulltext or len(fulltext.strip()) < 50:
                fulltext = preview
            summary = summarize(fulltext)
            keywords = extract_keywords(fulltext)
            tone = analyze_tone(fulltext)
            tags = generate_tags(fulltext)
            articles.append({
                "í‚¤ì›Œë“œ": keyword, "ì œëª©": title, "ë§í¬": link, "ë‚ ì§œ": date,
                "ë³¸ë¬¸": fulltext, "ìš”ì•½": summary, "í‚¤ì›Œë“œì¶”ì¶œ": keywords,
                "ì½˜í…ì¸ í†¤": tone, "íƒœê·¸": tags
            })
        except:
            continue
    return pd.DataFrame(articles)

# ìˆ˜ì§‘
lang_code = "ko" if lang_option == "í•œêµ­ì–´" else "en"
df_list = [fetch_news(k, lang=lang_code, max_items=max_items) for k in selected_keywords]
if df_list:
    news_df = pd.concat(df_list).drop_duplicates(subset=["ë§í¬"])
else:
    news_df = pd.DataFrame()

# ë‚ ì§œ í•„í„°
if not news_df.empty:
    news_df["ë‚ ì§œ"] = pd.to_datetime(news_df["ë‚ ì§œ"], errors="coerce")
    if start_date:
        news_df = news_df[news_df["ë‚ ì§œ"] >= pd.to_datetime(start_date)]
    if end_date:
        news_df = news_df[news_df["ë‚ ì§œ"] <= pd.to_datetime(end_date)]

# íƒ­ êµ¬ì„±
tab1, tab2, tab3 = st.tabs(["ğŸ“° ë‰´ìŠ¤ ëª©ë¡ ë³´ê¸°", "ğŸ“Š í†µê³„ ì‹œê°í™”", "ğŸ“ ë¶ë§ˆí¬ ë° PDF"])

with tab1:
    st.subheader("ğŸ“° ìˆ˜ì§‘ëœ ë‰´ìŠ¤")
    if not news_df.empty:
        if "bookmarks" not in st.session_state:
            st.session_state["bookmarks"] = []

        for i, row in news_df.iterrows():
            unique_key = f"{hash(row['ë§í¬'])}"
            st.markdown(f"### [{row['ì œëª©']}]({row['ë§í¬']})")
            st.caption(f"ğŸ“… {row['ë‚ ì§œ'].date()} | í†¤: `{row['ì½˜í…ì¸ í†¤']}` | {row['íƒœê·¸']}")
            st.markdown(f"ğŸ§¾ ìš”ì•½: {row['ìš”ì•½']}")
            st.markdown(f"`{row['í‚¤ì›Œë“œì¶”ì¶œ']}`")
            if st.button("â­ ë¶ë§ˆí¬", key=f"bookmark_{unique_key}"):
                if row["ë§í¬"] not in st.session_state["bookmarks"]:
                    st.session_state["bookmarks"].append(row["ë§í¬"])

    else:
        st.info("ë‰´ìŠ¤ ë°ì´í„°ë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")

with tab2:
    st.subheader("ğŸ“Š ë‰´ìŠ¤ í†µê³„ ë° ì›Œë“œí´ë¼ìš°ë“œ")
    if not news_df.empty:
        col1, col2 = st.columns([2, 1])
        with col1:
            st.markdown("#### í‚¤ì›Œë“œë³„ ë‰´ìŠ¤ ìˆ˜")
            st.bar_chart(news_df["í‚¤ì›Œë“œ"].value_counts())

            st.markdown("#### ì½˜í…ì¸  í†¤ ë¶„í¬")
            st.plotly_chart(px.histogram(news_df, x="í‚¤ì›Œë“œ", color="ì½˜í…ì¸ í†¤", barmode="group"))

        with col2:
            st.markdown("#### â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")
            all_kws = ", ".join(news_df["í‚¤ì›Œë“œì¶”ì¶œ"].dropna())
            try:
                font_path = "C:/Windows/Fonts/malgun.ttf"
                wc = WordCloud(font_path=font_path, width=400, height=300, background_color='white').generate(all_kws)
                fig, ax = plt.subplots()
                ax.imshow(wc, interpolation='bilinear')
                ax.axis("off")
                st.pyplot(fig)
            except:
                st.warning("âš ï¸ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨. í°íŠ¸ ê²½ë¡œ í™•ì¸ í•„ìš”")
    else:
        st.info("ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

with tab3:
    st.subheader("ğŸ“ ë¶ë§ˆí¬ ë° PDF ì €ì¥")
    if "bookmarks" in st.session_state:
        bm_df = news_df[news_df["ë§í¬"].isin(st.session_state["bookmarks"])]
        if not bm_df.empty:
            for _, row in bm_df.iterrows():
                st.markdown(f"- [{row['ì œëª©']}]({row['ë§í¬']})")
            if st.button("â¬‡ï¸ PDF ë‹¤ìš´ë¡œë“œ"):
                pdf = FPDF()
                pdf.add_page()
                font_path = "C:/Windows/Fonts/malgun.ttf"
                pdf.add_font('Malgun', '', font_path, uni=True)
                pdf.set_font('Malgun', '', 12)
                pdf.cell(200, 10, "ë¶ë§ˆí¬ ë‰´ìŠ¤ ìš”ì•½", 0, 1, 'C')

                for _, row in bm_df.iterrows():
                    entry = (
                        f"ğŸ“° ì œëª©: {row['ì œëª©']}\n"
                        f"ğŸ§¾ ìš”ì•½: {row['ìš”ì•½']}\n"
                        f"ğŸ“š ë³¸ë¬¸: {row['ë³¸ë¬¸'][:300]}...\n"
                        f"ğŸ·ï¸ í‚¤ì›Œë“œ: {row['í‚¤ì›Œë“œì¶”ì¶œ']}\n"
                        f"ğŸŒ€ í†¤: {row['ì½˜í…ì¸ í†¤']}\n"
                        f"ğŸ”– íƒœê·¸: {row['íƒœê·¸']}\n"
                        f"ğŸ”— ë§í¬: {row['ë§í¬']}\n"
                        + "-"*50 + "\n"
                    )
                    pdf.multi_cell(0, 10, entry)

                temp = BytesIO()
                pdf.output(temp)
                temp.seek(0)
                st.download_button("ğŸ“¥ PDF ì €ì¥í•˜ê¸°", data=temp.read(), file_name="ë¶ë§ˆí¬_ë‰´ìŠ¤.pdf")
        else:
            st.info("ğŸ“Œ ë¶ë§ˆí¬ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")