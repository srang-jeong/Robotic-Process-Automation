import streamlit as st
import pandas as pd
import feedparser
from urllib.parse import quote
from bs4 import BeautifulSoup
from konlpy.tag import Okt
from sentence_transformers import SentenceTransformer, util
from transformers import pipeline
from wordcloud import WordCloud
import matplotlib.pyplot as plt
import plotly.express as px
from datetime import datetime
from newspaper import Article
import nltk
from fpdf import FPDF
from io import BytesIO

nltk.download('punkt')

# ğŸ› ï¸ ëª¨ë¸ ë¡œë”©
@st.cache_resource
def load_models():
    summarizer = SentenceTransformer("jhgan/ko-sroberta-multitask")
    sentiment_ko = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")
    okt = Okt()
    return summarizer, sentiment_ko, okt

summarizer, sentiment_ko, okt = load_models()

# ğŸ“‹ ì´ëª¨ì§€ ë§¤í•‘
SENTI_EMOJI = {"ê¸ì •": "ğŸŸ¢", "ë¶€ì •": "ğŸ”´", "ì¤‘ë¦½": "ğŸŸ¡"}
TONE_EMOJI = {"ì •ë³´ì„±": "â„¹ï¸", "ê°ì •ì ": "ğŸ’¬", "ë¶„ì„ì ": "ğŸ§"}

# ğŸŒ í˜ì´ì§€ ì„¤ì •
st.set_page_config(page_title="ğŸ§  AI ë‰´ìŠ¤ ìš”ì•½ ëŒ€ì‹œë³´ë“œ", layout="wide")
st.title("ğŸ“° AI/ë¡œë´‡ ë‰´ìŠ¤ ìš”ì•½ ëŒ€ì‹œë³´ë“œ")

# ğŸ›ï¸ ì‚¬ì´ë“œë°” ì…ë ¥
st.sidebar.header("ğŸ” ë‰´ìŠ¤ ìˆ˜ì§‘ ì¡°ê±´")
KEYWORDS = ["AI", "ë¡œë´‡", "ë¡œë´‡ê°ì •", "ë¡œë´‡ì„±ê²©", "IT", "ì‚°ì—…ë°ì´í„°", "ë°ì´í„°ì‹œìŠ¤í…œ"]
selected_keywords = st.sidebar.multiselect("ğŸ’¡ í‚¤ì›Œë“œ ì„ íƒ", KEYWORDS, default=KEYWORDS)
extra_kw = st.sidebar.text_input("â• ì¶”ê°€ í‚¤ì›Œë“œ (ì‰¼í‘œ êµ¬ë¶„)")
if extra_kw:
    selected_keywords += [kw.strip() for kw in extra_kw.split(",") if kw.strip()]
lang_option = st.sidebar.radio("ğŸŒ ë‰´ìŠ¤ ì–¸ì–´", ["í•œêµ­ì–´", "ì˜ì–´"])
max_items = st.sidebar.slider("ğŸ“° í‚¤ì›Œë“œë³„ ë‰´ìŠ¤ ìˆ˜", 1, 15, 5)
start_date = st.sidebar.date_input("ğŸ“… ì‹œì‘ì¼", None)
end_date = st.sidebar.date_input("ğŸ“… ì¢…ë£Œì¼", None)

# ğŸ“‘ ë‰´ìŠ¤ ë³¸ë¬¸ ì •ì œ
def clean_text(html):
    return BeautifulSoup(html, "html.parser").get_text(separator=" ").strip()

# âœ‚ï¸ ì„ë² ë”© ê¸°ë°˜ ìš”ì•½
def summarize(text, num_sent=3):
    if not text or len(text.strip()) < 30:
        return "ìš”ì•½ ë¶ˆê°€ (ë³¸ë¬¸ ë¶€ì¡±)"
    sentences = [s.strip() for s in text.replace("!", ".").split(". ") if len(s.strip()) > 10]
    if len(sentences) <= num_sent:
        return text
    emb = summarizer.encode(sentences, convert_to_tensor=True)
    center = emb.mean(dim=0)
    scores = [util.pytorch_cos_sim(center, e)[0][0].item() for e in emb]
    top_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:num_sent]
    return ". ".join([sentences[i] for i in sorted(top_idx)])

# ğŸ·ï¸ í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords(text, n=5):
    nouns = [w for w in okt.nouns(text) if len(w) > 1]
    freq = pd.Series(nouns).value_counts()
    return ", ".join(freq.head(n).index) if not freq.empty else "í‚¤ì›Œë“œ ì—†ìŒ"

# ğŸ˜¶ ê°ì„± ë¶„ì„
def get_sentiment(text):
    try:
        result = sentiment_ko(text[:512])
        label = result[0]["label"]
        if "1" in label or "2" in label:
            return "ë¶€ì •"
        elif "4" in label or "5" in label:
            return "ê¸ì •"
        else:
            return "ì¤‘ë¦½"
    except:
        return "ì¤‘ë¦½"

# ğŸ§ ì½˜í…ì¸  í†¤ ë¶„ì„
def analyze_tone(text):
    if len(text) > 1000 and any(word in text for word in ["ìš°ë ¤", "ë…¼ë€", "í™˜ì˜", "ë…¼ì˜", "ê³¼ì œ"]):
        return "ë¶„ì„ì "
    elif any(word in text for word in ["ìŠ¬í”„ë‹¤", "ê¸°ì˜ë‹¤", "ì¶©ê²©", "ê°ë™", "ë¶„ë…¸"]):
        return "ê°ì •ì "
    else:
        return "ì •ë³´ì„±"

# ğŸ·ï¸ íƒœê·¸ ìƒì„±
def generate_tags(text):
    tags = []
    if "ê¸°ìˆ " in text:
        tags.append("#ê¸°ìˆ ë™í–¥")
    if "ì‹œì¥" in text or "ìˆ˜ìš”" in text:
        tags.append("#ì‹œì¥ë¶„ì„")
    if "ë…¼ë€" in text or "ë¬¸ì œ" in text:
        tags.append("#ì´ìŠˆ")
    return " ".join(tags) if tags else "#ì¼ë°˜"

# ğŸ’¡ í•œì¤„í‰ ìƒì„± (ì´ëª¨ì§€ í¬í•¨)
def generate_opinion(sentiment, tone):
    senti_txt = {
        "ê¸ì •": "ğŸŸ¢ ê¸ì •ì ì¸ ê´€ì ",
        "ë¶€ì •": "ğŸ”´ ë¹„íŒì ì¸ ê´€ì ",
        "ì¤‘ë¦½": "ğŸŸ¡ ì¤‘ë¦½ì ì¸ ê´€ì "
    }.get(sentiment, "ğŸŸ¡ ì¤‘ë¦½ì ì¸ ê´€ì ")
    tone_txt = {
        "ì •ë³´ì„±": "â„¹ï¸ ì •ë³´ ì „ë‹¬",
        "ê°ì •ì ": "ğŸ’¬ ê°ì • í‘œí˜„",
        "ë¶„ì„ì ": "ğŸ§ ë¶„ì„ì  ì ‘ê·¼"
    }.get(tone, "â„¹ï¸ ì •ë³´ ì „ë‹¬")
    return f"{senti_txt} + {tone_txt}ì˜ ë‰´ìŠ¤ì…ë‹ˆë‹¤."

# ğŸ“° ë‰´ìŠ¤ ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
@st.cache_data(show_spinner=True)
def get_article_text(url, lang="ko"):
    try:
        article = Article(url, language='ko' if lang == "í•œêµ­ì–´" else 'en')
        article.download()
        article.parse()
        return article.text.strip()
    except:
        return ""

# ğŸŒ ë‰´ìŠ¤ í¬ë¡¤ë§
@st.cache_data(show_spinner=True)
def fetch_news(keyword, lang="ko", max_items=5):
    q = quote(keyword)
    rss_url = f"https://news.google.com/rss/search?q={q}&hl={lang}&gl=KR&ceid=KR:{lang}"
    feed = feedparser.parse(rss_url)
    articles = []
    for entry in feed.entries[:max_items]:
        try:
            title = entry.title
            link = entry.link
            date = entry.get("published", datetime.now().strftime("%Y-%m-%d"))
            preview = clean_text(entry.summary if hasattr(entry, "summary") else "")
            fulltext = get_article_text(link, lang)
            if not fulltext or len(fulltext.strip()) < 50:
                fulltext = preview
            summary = summarize(fulltext)
            keywords = extract_keywords(fulltext)
            sentiment = get_sentiment(fulltext)
            tone = analyze_tone(fulltext)
            tags = generate_tags(fulltext)
            opinion = generate_opinion(sentiment, tone)
            articles.append({
                "í‚¤ì›Œë“œ": keyword, "ì œëª©": title, "ë§í¬": link, "ë‚ ì§œ": date,
                "ë³¸ë¬¸": fulltext, "ìš”ì•½": summary, "í‚¤ì›Œë“œì¶”ì¶œ": keywords,
                "ê°ì„±": sentiment, "ì½˜í…ì¸ í†¤": tone, "íƒœê·¸": tags, "í•œì¤„í‰": opinion
            })
        except:
            continue
    return pd.DataFrame(articles)

# ğŸš€ ë‰´ìŠ¤ ë°ì´í„° ìˆ˜ì§‘
lang_code = "ko" if lang_option == "í•œêµ­ì–´" else "en"
df_list = [fetch_news(k, lang=lang_code, max_items=max_items) for k in selected_keywords]
news_df = pd.concat(df_list).drop_duplicates(subset=["ë§í¬"]) if df_list else pd.DataFrame()

# ğŸ•’ ë‚ ì§œ í•„í„°
if not news_df.empty:
    news_df["ë‚ ì§œ"] = pd.to_datetime(news_df["ë‚ ì§œ"], errors="coerce")
    if start_date:
        news_df = news_df[news_df["ë‚ ì§œ"] >= pd.to_datetime(start_date)]
    if end_date:
        news_df = news_df[news_df["ë‚ ì§œ"] <= pd.to_datetime(end_date)]

# ğŸ—‚ï¸ íƒ­ êµ¬ì„±
tab1, tab2, tab3 = st.tabs(["ğŸ“° ë‰´ìŠ¤ ëª©ë¡", "ğŸ“Š í†µê³„Â·ì›Œë“œí´ë¼ìš°ë“œ", "ğŸ“ ë¶ë§ˆí¬/PDF"])

# ğŸ“° íƒ­1: ë‰´ìŠ¤ ëª©ë¡
with tab1:
    st.subheader("ğŸ“° ìµœì‹  ë‰´ìŠ¤ ëª©ë¡")
    if not news_df.empty:
        if "bookmarks" not in st.session_state:
            st.session_state["bookmarks"] = []
        for i, row in news_df.iterrows():
            senti_emo = SENTI_EMOJI.get(row["ê°ì„±"], "ğŸŸ¡")
            tone_emo = TONE_EMOJI.get(row["ì½˜í…ì¸ í†¤"], "â„¹ï¸")
            st.markdown(
                f"### {senti_emo}{tone_emo} [{row['ì œëª©']}]({row['ë§í¬']})"
            )
            st.caption(
                f"ğŸ“… {row['ë‚ ì§œ'].date()} | {senti_emo} ê°ì„±: `{row['ê°ì„±']}` | {tone_emo} í†¤: `{row['ì½˜í…ì¸ í†¤']}` | {row['íƒœê·¸']}"
            )
            st.markdown(f"ğŸ§¾ **ìš”ì•½:** {row['ìš”ì•½']}")
            st.markdown(f"ğŸ’¡ **í•œì¤„í‰:** {row['í•œì¤„í‰']}")
            st.markdown(f"ğŸ·ï¸ `{row['í‚¤ì›Œë“œì¶”ì¶œ']}`")
            if st.button("â­ ë¶ë§ˆí¬", key=f"bookmark_{hash(row['ë§í¬'])}"):
                if row["ë§í¬"] not in st.session_state["bookmarks"]:
                    st.session_state["bookmarks"].append(row["ë§í¬"])
            st.divider()
    else:
        st.info("ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ğŸ“Š íƒ­2: í†µê³„ ë° ì›Œë“œí´ë¼ìš°ë“œ
with tab2:
    st.subheader("ğŸ“Š ë‰´ìŠ¤ í†µê³„ ë° ì›Œë“œí´ë¼ìš°ë“œ")
    if not news_df.empty:
        col1, col2 = st.columns([2, 1])
        with col1:
            st.markdown("#### ğŸ”¢ í‚¤ì›Œë“œë³„ ë‰´ìŠ¤ ìˆ˜")
            st.bar_chart(news_df["í‚¤ì›Œë“œ"].value_counts())

            st.markdown("#### ğŸ˜¶ ê°ì„± ë¶„í¬")
            senti_bar = news_df["ê°ì„±"].value_counts().rename_axis('ê°ì„±').reset_index(name='ê±´ìˆ˜')
            senti_bar["ì´ëª¨ì§€"] = senti_bar["ê°ì„±"].map(SENTI_EMOJI)
            st.dataframe(senti_bar, hide_index=True)
            st.bar_chart(news_df["ê°ì„±"].value_counts())

            st.markdown("#### ğŸ§ ì½˜í…ì¸  í†¤ ë¶„í¬")
            st.plotly_chart(px.histogram(news_df, x="í‚¤ì›Œë“œ", color="ì½˜í…ì¸ í†¤", barmode="group"))

        with col2:
            st.markdown("#### â˜ï¸ ì›Œë“œí´ë¼ìš°ë“œ")
            all_kws = ", ".join(news_df["í‚¤ì›Œë“œì¶”ì¶œ"].dropna())
            try:
                font_path = "C:/Windows/Fonts/malgun.ttf"
                wc = WordCloud(font_path=font_path, width=400, height=300, background_color='white').generate(all_kws)
                fig, ax = plt.subplots()
                ax.imshow(wc, interpolation='bilinear')
                ax.axis("off")
                st.pyplot(fig)
            except:
                st.warning("âš ï¸ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨. í°íŠ¸ ê²½ë¡œë¥¼ í™•ì¸í•´ì£¼ì„¸ìš”.")
    else:
        st.info("ì‹œê°í™”í•  ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")

# ğŸ“ íƒ­3: ë¶ë§ˆí¬ & PDF
with tab3:
    st.subheader("ğŸ“ ë¶ë§ˆí¬ ë° PDF ì €ì¥")
    bm_df = news_df[news_df["ë§í¬"].isin(st.session_state.get("bookmarks", []))]
    if not bm_df.empty:
        for _, row in bm_df.iterrows():
            st.markdown(f"- {SENTI_EMOJI.get(row['ê°ì„±'],'ğŸŸ¡')}{TONE_EMOJI.get(row['ì½˜í…ì¸ í†¤'],'â„¹ï¸')} [{row['ì œëª©']}]({row['ë§í¬']})")
        if st.button("â¬‡ï¸ PDF ë‹¤ìš´ë¡œë“œ"):
            pdf = FPDF()
            pdf.add_page()
            font_path = "C:/Windows/Fonts/malgun.ttf"
            pdf.add_font('Malgun', '', font_path, uni=True)
            pdf.set_font('Malgun', '', 12)
            pdf.cell(200, 10, "â­ ë¶ë§ˆí¬ ë‰´ìŠ¤ ìš”ì•½", 0, 1, 'C')
            for _, row in bm_df.iterrows():
                entry = (
                    f"{SENTI_EMOJI.get(row['ê°ì„±'],'ğŸŸ¡')}{TONE_EMOJI.get(row['ì½˜í…ì¸ í†¤'],'â„¹ï¸')} ì œëª©: {row['ì œëª©']}\n"
                    f"ìš”ì•½: {row['ìš”ì•½']}\n"
                    f"ê°ì„±: {row['ê°ì„±']}, í†¤: {row['ì½˜í…ì¸ í†¤']}\n"
                    f"í•œì¤„í‰: {row['í•œì¤„í‰']}\n"
                    f"í‚¤ì›Œë“œ: {row['í‚¤ì›Œë“œì¶”ì¶œ']}\n"
                    f"íƒœê·¸: {row['íƒœê·¸']}\n"
                    f"ë§í¬: {row['ë§í¬']}\n"
                    + "-"*40 + "\n"
                )
                pdf.multi_cell(0, 10, entry)
            temp = BytesIO()
            pdf.output(temp)
            temp.seek(0)
            st.download_button("ğŸ“„ PDF ì €ì¥", data=temp.read(), file_name="bookmarked_news.pdf")
    else:
        st.info("â­ ë¶ë§ˆí¬ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")