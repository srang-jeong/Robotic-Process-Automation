import streamlit as st
import pandas as pd
import feedparser
from urllib.parse import quote
from bs4 import BeautifulSoup
from konlpy.tag import Okt
from sentence_transformers import SentenceTransformer, util
from transformers import pipeline
from wordcloud import WordCloud
from io import BytesIO
import matplotlib.pyplot as plt
from datetime import datetime
from newspaper import Article
import nltk
from fpdf import FPDF
import matplotlib.font_manager as fm

nltk.download('punkt')

# ëª¨ë¸ ë¡œë”©
@st.cache_resource
def load_models():
    summarizer = SentenceTransformer("jhgan/ko-sroberta-multitask")
    sentiment_ko = pipeline("sentiment-analysis", model="nlptown/bert-base-multilingual-uncased-sentiment")
    sentiment_en = pipeline("sentiment-analysis", model="distilbert-base-uncased-finetuned-sst-2-english")
    okt = Okt()
    return summarizer, sentiment_ko, sentiment_en, okt

summarizer, sentiment_ko, sentiment_en, okt = load_models()

# ì´ˆê¸°í™”
st.set_page_config(page_title="AI ë‰´ìŠ¤ ëŒ€ì‹œë³´ë“œ", layout="wide")
st.title("ğŸ§ ğŸ› ï¸ğŸ•¹ï¸ AI/ë¡œë´‡ ë‰´ìŠ¤ ìš”ì•½ ëŒ€ì‹œë³´ë“œ")

# ì‚¬ì´ë“œë°” ì…ë ¥
with st.sidebar:
    st.header("ğŸ› ï¸ ë‰´ìŠ¤ ìˆ˜ì§‘ ì¡°ê±´")
    KEYWORDS = ["AI", "ë¡œë´‡", "ë¡œë´‡ê°ì •", "ë¡œë´‡ì„±ê²©", "IT", "ì‚°ì—…ë°ì´í„°", "ë°ì´í„°ì‹œìŠ¤í…œ"]
    selected_keywords = st.multiselect("ğŸ” ê´€ì‹¬ í‚¤ì›Œë“œ", KEYWORDS, default=["AI", "ë¡œë´‡", "ë¡œë´‡ê°ì •", "ë¡œë´‡ì„±ê²©", "IT", "ì‚°ì—…ë°ì´í„°", "ë°ì´í„°ì‹œìŠ¤í…œ"])
    extra_kw = st.text_input("â• ì¶”ê°€ í‚¤ì›Œë“œ (ì‰¼í‘œë¡œ êµ¬ë¶„)")
    if extra_kw:
        selected_keywords += [kw.strip() for kw in extra_kw.split(",") if kw.strip()]
    lang_option = st.radio("ğŸŒ ë‰´ìŠ¤ ì–¸ì–´", ["í•œêµ­ì–´", "ì˜ì–´"])
    max_items = st.slider("ğŸ“° í‚¤ì›Œë“œë³„ ìµœëŒ€ ë‰´ìŠ¤ ìˆ˜", 1, 15, 5)
    start_date = st.date_input("ğŸ“… ì‹œì‘ì¼", None)
    end_date = st.date_input("ğŸ“… ì¢…ë£Œì¼", None)

lang_code = "ko" if lang_option == "í•œêµ­ì–´" else "en"
senti_emoji = {"ê¸ì •": "ğŸŸ¢", "ë¶€ì •": "ğŸ”´", "ì¤‘ë¦½": "ğŸŸ¡"}

# ê°ì„± ìš”ì•½
def generate_opinion(summary, sentiment):
    base = "ì´ ë‰´ìŠ¤ëŠ” "
    if "ê¸ì •" in sentiment:
        return base + "ê¸ì •ì ì¸ ë¶„ìœ„ê¸°ë¥¼ ì „ë‹¬í•©ë‹ˆë‹¤."
    elif "ë¶€ì •" in sentiment:
        return base + "ë¹„íŒì  ì‹œê°ì„ ë³´ì—¬ì¤ë‹ˆë‹¤."
    else:
        return base + "ì¤‘ë¦½ì ì¸ ì •ë³´ë¥¼ ì œê³µí•©ë‹ˆë‹¤."

# ë³¸ë¬¸ ì •ì œ
def clean_text(html):
    return BeautifulSoup(html, "html.parser").get_text(separator=" ").strip()

# ìš”ì•½
def summarize(text, num_sent=3):
    if not text or len(text.strip()) < 30:
        return "ìš”ì•½ ë¶ˆê°€ (ë³¸ë¬¸ ë¶€ì¡±)"
    sentences = [s.strip() for s in text.replace("!", ".").split(". ") if len(s.strip()) > 10]
    if len(sentences) <= num_sent:
        return text
    emb = summarizer.encode(sentences, convert_to_tensor=True)
    center = emb.mean(dim=0)
    scores = [util.pytorch_cos_sim(center, e)[0][0].item() for e in emb]
    top_idx = sorted(range(len(scores)), key=lambda i: scores[i], reverse=True)[:num_sent]
    return ". ".join([sentences[i] for i in sorted(top_idx)])

# í‚¤ì›Œë“œ ì¶”ì¶œ
def extract_keywords(text, n=5):
    nouns = [w for w in okt.nouns(text) if len(w) > 1]
    freq = pd.Series(nouns).value_counts()
    return ", ".join(freq.head(n).index) if not freq.empty else "í‚¤ì›Œë“œ ì—†ìŒ"

# ê°ì„±ë¶„ì„
def get_sentiment(text, lang="ko"):
    try:
        if lang == "ko":
            result = sentiment_ko(text[:256])
            return result[0]["label"]
        else:
            result = sentiment_en(text)
            return {"POSITIVE": "ê¸ì •", "NEGATIVE": "ë¶€ì •"}.get(result[0]["label"], "ì¤‘ë¦½")
    except:
        return "ì¤‘ë¦½"

# ë³¸ë¬¸ ê°€ì ¸ì˜¤ê¸°
@st.cache_data
def get_article_text(url, lang="ko"):
    try:
        article = Article(url, language='ko' if lang == "ko" else 'en')
        article.download()
        article.parse()
        return article.text.strip()
    except:
        return ""

# ë‰´ìŠ¤ ìˆ˜ì§‘
@st.cache_data
def fetch_news(keyword, lang="ko"):
    q = quote(keyword)
    rss_url = f"https://news.google.com/rss/search?q={q}&hl={lang}&gl=KR&ceid=KR:{lang}"
    feed = feedparser.parse(rss_url)
    articles = []
    for entry in feed.entries[:max_items]:
        try:
            title = entry.title
            link = entry.link
            date = entry.get("published", datetime.now().strftime("%Y-%m-%d"))
            preview = clean_text(entry.summary if hasattr(entry, "summary") else "")
            fulltext = get_article_text(link, lang)
            if not fulltext or len(fulltext.strip()) < 50:
                fulltext = preview
            summary = summarize(fulltext)
            keywords = extract_keywords(fulltext)
            senti = get_sentiment(fulltext, lang)
            opinion = generate_opinion(summary, senti)
            articles.append({
                "í‚¤ì›Œë“œ": keyword, "ì œëª©": title, "ë§í¬": link, "ë‚ ì§œ": date,
                "ë³¸ë¬¸": fulltext, "ìš”ì•½": summary, "í‚¤ì›Œë“œì¶”ì¶œ": keywords,
                "ê°ì„±": senti, "í•œì¤„í‰": opinion
            })
        except:
            continue
    return pd.DataFrame(articles)

# ë°ì´í„° ìˆ˜ì§‘ ë° ì²˜ë¦¬
df_list = [fetch_news(k, lang=lang_code) for k in selected_keywords]
news_df = pd.concat(df_list).drop_duplicates(subset=["ë§í¬"]) if df_list else pd.DataFrame()
news_df["ë‚ ì§œ"] = pd.to_datetime(news_df["ë‚ ì§œ"], errors="coerce")
news_df["ê°ì„±ì´ëª¨ì§€"] = news_df["ê°ì„±"].map(senti_emoji)

# ë¶ë§ˆí¬ ì´ˆê¸°í™”
if "bookmarks" not in st.session_state:
    st.session_state["bookmarks"] = []

# íƒ­ êµ¬ì„±
tab1, tab2, tab3 = st.tabs(["ğŸ“° ë‰´ìŠ¤ ëª©ë¡ ë³´ê¸°", "ğŸ“Š í†µê³„ ì‹œê°í™”", "ğŸ“ ë¶ë§ˆí¬ & PDF ì €ì¥"])

with tab1:
    st.subheader("ğŸ“° ë‰´ìŠ¤ ëª©ë¡")
    if news_df.empty:
        st.warning("ë‰´ìŠ¤ ë°ì´í„°ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        for i, row in news_df.iterrows():
            row_key = str(hash(row["ë§í¬"]))
            st.markdown(f"### [{row['ì œëª©']}]({row['ë§í¬']})")
            st.caption(f"ğŸ“… {row['ë‚ ì§œ'].date()}")
            st.markdown(f"ğŸ” í‚¤ì›Œë“œ ì¶”ì¶œ: `{row['í‚¤ì›Œë“œì¶”ì¶œ']}`")
            st.markdown(f"ğŸ“‹ í•œì¤„í‰: {row['í•œì¤„í‰']}")
            st.markdown(f"{senti_emoji.get(row['ê°ì„±'], 'ğŸŸ¡')} ê°ì„±: {row['ê°ì„±']}")
            if st.button("â­ ë¶ë§ˆí¬", key=f"bookmark_{row_key}"):
                if row["ë§í¬"] not in st.session_state["bookmarks"]:
                    st.session_state["bookmarks"].append(row["ë§í¬"])
            st.divider()

with tab2:
    st.subheader("ğŸ“Š í†µê³„ ì‹œê°í™”")
    if news_df.empty:
        st.warning("ë°ì´í„° ì—†ìŒ")
    else:
        st.markdown("### í‚¤ì›Œë“œë³„ ë‰´ìŠ¤ ê±´ìˆ˜")
        st.bar_chart(news_df["í‚¤ì›Œë“œ"].value_counts())

        st.markdown("### ê°ì„±ë³„ ë‰´ìŠ¤ ë¶„í¬")
        st.bar_chart(news_df["ê°ì„±"].value_counts())

        st.markdown("### ì›Œë“œí´ë¼ìš°ë“œ")
        all_kws = ", ".join(news_df["í‚¤ì›Œë“œì¶”ì¶œ"].dropna())
        try:
            font_path = "C:/Windows/Fonts/malgun.ttf"
            wc = WordCloud(font_path=font_path, background_color='white', width=800, height=400).generate(all_kws)
            fig, ax = plt.subplots()
            ax.imshow(wc, interpolation="bilinear")
            ax.axis("off")
            st.pyplot(fig)
        except:
            st.warning("âš ï¸ ì›Œë“œí´ë¼ìš°ë“œ ìƒì„± ì‹¤íŒ¨")

with tab3:
    st.subheader("ğŸ“ ë¶ë§ˆí¬ëœ ë‰´ìŠ¤ PDF ì €ì¥")
    bm_df = news_df[news_df["ë§í¬"].isin(st.session_state["bookmarks"])]
    if bm_df.empty:
        st.info("ë¶ë§ˆí¬ëœ ë‰´ìŠ¤ê°€ ì—†ìŠµë‹ˆë‹¤.")
    else:
        for _, row in bm_df.iterrows():
            st.markdown(f"- [{row['ì œëª©']}]({row['ë§í¬']})")
        if st.button("â¬‡ï¸ PDF ë‹¤ìš´ë¡œë“œ"):
            pdf = FPDF()
            pdf.add_page()
            font_path = "C:/Windows/Fonts/malgun.ttf"
            pdf.add_font('Malgun', '', font_path, uni=True)
            pdf.set_font("Malgun", "", 12)
            pdf.cell(200, 10, "ë¶ë§ˆí¬ ë‰´ìŠ¤ ëª©ë¡", ln=1, align='C')
            for _, row in bm_df.iterrows():
                entry = f"ì œëª©: {row['ì œëª©']}\nìš”ì•½: {row['ìš”ì•½']}\ní•œì¤„í‰: {row['í•œì¤„í‰']}\në§í¬: {row['ë§í¬']}\n\n"
                pdf.multi_cell(0, 10, entry)
            temp = BytesIO()
            pdf.output(temp)
            temp.seek(0)
            st.download_button("ğŸ“„ PDF ì €ì¥", data=temp.read(), file_name="bookmarked_news.pdf")